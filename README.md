# Evaluating LLM Semantic Profiling Capabilities

Contains the codebase used for prompting LLMs and performing comparisons with human annotations.We evaluated two proprietary and two open-source LLMs.

**Proprietary LLMs**. We evaluated OpenAI's GPT4-Turbo and Google's Gemini-Pro. GPT4-Turbo has a training data cutoff of December 2023 and Gemini-Pro's training data cutoff is described as "early 2023" [According to Google AI documentation](https://anvilproject.org/guides/content/creating-links). We utilized the Application Programming Interfaces (APIs) for both of these models to generate responses for the 500 utterances in our corpus.

**Open Source LLMs**. We evaluated two open-source LLMs, Llama3, and Mixtral, on the Llama factory code base. Llama3 has 70 billion parameters and a context length of 8,000 tokens, with a knowledge cutoff of December 2023. Mixtral-8x7B-Instruct is configured with 46.7 billion parameters and similarly has a knowledge cutoff in December 2023.

Our experimental setup for the open-source models involved utilizing an NVIDIA H100 GPU coupled with a 48-core Intel Sapphire Rapids CPU, supported by 100GB of system memory. Both models were operational in 4-bit quantization mode and were enhanced with flash attention mechanisms to expedite the inference process. The inference duration for the LLAMA3 model was approximately 2 hours, whereas for the Mixtral model, it extended to about 3 hours.

## Instructions
To insure you have all the required packages for this project please run `pip install -r ./requirements.txt` in the terminal. You should now be able to run the necessary scripts in this repo.

## Content
This project contains the following folders

**Datasets:** contains all 37 datasets used by the 500 utterances used in this study. All files are .csv

**GPT_Gemini_Prompting_Scripts:** Contains the scripts used to prompt the proprietary LLMs (GPT4-turbo and Gemini-Pro) evaluated in this study. 

**Llama_Mixtral_Prompting_Scripts:** Contains the scripts used to prompt the open source LLMs evaluated in this study. Also contaions json results from llama and mixtral runs.

**Output Analysis:** Contains the scripts used to evaluate the responses generated by all LLMs evaluated in this study


